{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDUVA7WCIWde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zlib import crc32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ecEqR2qIWdl",
        "colab_type": "code",
        "outputId": "adafe6ab-aac0-422c-8176-5f4ce4936d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "crc32('грабовой'.lower().encode('utf-8'))%5+1, crc32('grabovoy'.lower().encode('utf-8'))%3+1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRw7tB2F_kvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5mJHz-6BGj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-xH14pMIWdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "\n",
        "from torchvision import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYh-uWuv-xsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezKBGTrTIWdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOVW8XKdIWd2",
        "colab_type": "code",
        "outputId": "72a25541-26db-4000-8817-359f6e5dbd2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "MNIST = datasets.MNIST('./mnist', train=False, download=True, transform=None)\n",
        "X_test, Y_test = MNIST.test_data, MNIST.test_labels\n",
        "\n",
        "MNIST = datasets.MNIST('./mnist', train=True, download=True, transform=None)\n",
        "X_train, Y_train = MNIST.train_data, MNIST.train_labels"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MECG-LbtIWd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape([X_train.shape[0], -1]).float()\n",
        "X_test = X_test.reshape([X_test.shape[0], -1]).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM1Tt6gsIWd7",
        "colab_type": "code",
        "outputId": "0bf794f6-077e-441b-beef-948fddcfccb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 784]), torch.Size([10000, 784]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c50HSjxgIWd9",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IELWT_-JnExw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False):\n",
        "    '''\n",
        "    Compute the gradient of `outputs` with respect to `inputs`\n",
        "    gradient(x.sum(), x)\n",
        "    gradient((x * y).sum(), [x, y])\n",
        "    '''\n",
        "    if torch.is_tensor(inputs):\n",
        "        inputs = [inputs]\n",
        "    else:\n",
        "        inputs = list(inputs)\n",
        "    grads = torch.autograd.grad(outputs, inputs, grad_outputs,\n",
        "                                allow_unused=True,\n",
        "                                retain_graph=retain_graph,\n",
        "                                create_graph=create_graph)\n",
        "    grads = [x if x is not None else torch.zeros_like(y) for x, y in zip(grads, inputs)]\n",
        "    return torch.cat([x.contiguous().view(-1) for x in grads])\n",
        "\n",
        "def hessian(output, inputs, out=None, allow_unused=False, create_graph=False):\n",
        "    '''\n",
        "    Compute the Hessian of `output` with respect to `inputs`\n",
        "    hessian((x * y).sum(), [x, y])\n",
        "    '''\n",
        "    assert output.ndimension() == 0\n",
        "\n",
        "    if torch.is_tensor(inputs):\n",
        "        inputs = [inputs]\n",
        "    else:\n",
        "        inputs = list(inputs)\n",
        "\n",
        "    n = sum(p.numel() for p in inputs)\n",
        "    if out is None:\n",
        "        out = output.new_zeros(n, n)\n",
        "\n",
        "    ai = 0\n",
        "    for i, inp in enumerate(inputs):\n",
        "        [grad] = torch.autograd.grad(output, inp, create_graph=True, allow_unused=allow_unused)\n",
        "        grad = torch.zeros_like(inp) if grad is None else grad\n",
        "        grad = grad.contiguous().view(-1)\n",
        "\n",
        "        for j in range(inp.numel()):\n",
        "            if grad[j].requires_grad:\n",
        "                row = gradient(grad[j], inputs[i:], retain_graph=True, create_graph=create_graph)[j:]\n",
        "            else:\n",
        "                row = grad[j].new_zeros(sum(x.numel() for x in inputs[i:]) - j)\n",
        "\n",
        "            out[ai, ai:].add_(row.type_as(out))  # ai's row\n",
        "            if ai + 1 < n:\n",
        "                out[ai + 1:, ai].add_(row[1:].type_as(out))  # ai's column\n",
        "            del row\n",
        "            ai += 1\n",
        "        del grad\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sU4N4hkIWd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self, input_dim = 20, output_dim = 10, device = 'cpu'):\n",
        "        super(LogReg, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "        self.to(device)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = input\n",
        "        out = F.relu(out)\n",
        "        return self.linear(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL9H_7P_IWd_",
        "colab_type": "text"
      },
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veyRL9LKIWd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function):\n",
        "        model.zero_grad()\n",
        "        \n",
        "        output = model(batch_of_x)\n",
        "        \n",
        "        loss = loss_function(output, batch_of_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "     \n",
        "        return\n",
        "    \n",
        "def train_epoch(train_generator, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
        "        train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
        "\n",
        "    return\n",
        "\n",
        "def trainer(count_of_epoch, \n",
        "            batch_size, \n",
        "            dataset,\n",
        "            model, \n",
        "            loss_function,\n",
        "            optimizer,\n",
        "            progress = None\n",
        "           ):\n",
        "    iterations = range(count_of_epoch)\n",
        "    if progress is not None:\n",
        "        iterations = progress(iterations)\n",
        "\n",
        "    for it in iterations:\n",
        "        optima = optimizer\n",
        "\n",
        "        batch_generator = DataLoader(dataset = dataset, batch_size = batch_size, shuffle=True)\n",
        "        \n",
        "        train_epoch(train_generator = batch_generator, model = model, loss_function = loss_function, optimizer = optima)\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYhvcbLmIWeA",
        "colab_type": "text"
      },
      "source": [
        "# Переопределеный оптимизатор"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J43Um9tIWeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Adam(Optimizer):\n",
        "    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "            \n",
        "# ==================================================\n",
        "# Additition function\n",
        "    def get_masks(self, kind = 'prune'):\n",
        "        list_of_mask = []\n",
        "        LIST_prune = []\n",
        "        mask_vector = None\n",
        "        if kind == 'prune':\n",
        "            for group in self.param_groups:\n",
        "                 for p in group['params']:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'prune' not in param_state:\n",
        "                        param_state['prune'] = torch.ones_like(p)\n",
        "\n",
        "                    p_vector = p.view(-1)\n",
        "                    prune_vector = param_state['prune'].view(-1)\n",
        "                    LIST_prune.append(prune_vector)\n",
        "        mask_vector = torch.cat(LIST_prune)\n",
        "        return mask_vector\n",
        "\n",
        "    def prune(self, amount = 0.1, method='random', **argv):\n",
        "        if method == 'random':\n",
        "            for group in self.param_groups:\n",
        "                 for p in group['params']:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'prune' not in param_state:\n",
        "                        param_state['prune'] = torch.ones_like(p)\n",
        "\n",
        "                    p_vector = p.view(-1)\n",
        "                    prune_vector = param_state['prune'].view(-1)\n",
        "                    if int(amount*len(p_vector)) > 0:\n",
        "                        prune_vector[torch.randperm(p_vector.shape[0])[:int(amount*len(p_vector))]] = 0\n",
        "                        p_vector.data.mul_(prune_vector)\n",
        "\n",
        "        elif method == 'OBD':\n",
        "            if argv['hessian'] is None:\n",
        "                pass\n",
        "            else:\n",
        "                for group in self.param_groups:\n",
        "                    for p, h in zip(group['params'], argv['hessian']):\n",
        "                        param_state = self.state[p]\n",
        "                        if 'prune' not in param_state:\n",
        "                            param_state['prune'] = torch.ones_like(p)\n",
        "\n",
        "                        p_vector = p.view(-1)\n",
        "                        score = (p*h).view(-1)\n",
        "                        prune_vector = param_state['prune'].view(-1)\n",
        "                        if int(amount*len(p_vector)) > 0:\n",
        "                            prune_vector[torch.sort(score)[1][:int(amount*len(p_vector))]] = 0\n",
        "                            p_vector.data.mul_(prune_vector)\n",
        "\n",
        "            pass\n",
        "    \n",
        "    def deprune(self):\n",
        "        for group in self.param_groups:\n",
        "             for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                param_state['prune'] = torch.ones_like(p)\n",
        "# ==================================================\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                    \n",
        "# ==================================================\n",
        "# Addition in each step\n",
        "                d_p = p.grad.data.detach()\n",
        "                param_state = self.state[p]\n",
        "    \n",
        "                if 'prune' in param_state:\n",
        "                    prune = param_state['prune']\n",
        "                else:\n",
        "                    prune = torch.ones_like(p)\n",
        "                    param_state['prune'] = prune\n",
        "# ==================================================                   \n",
        "                    \n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if 'step' not in state:\n",
        "                    state['step'] = 0\n",
        "                # Exponential moving average of gradient values\n",
        "                if 'exp_avg' not in state:\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                # Exponential moving average of squared gradient values\n",
        "                if 'exp_avg_sq' not in state:\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                if amsgrad:\n",
        "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                    if 'max_exp_avg_sq' not in state:\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                \n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad.add_(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                else:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                    \n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                step_size = group['lr'] * (bias_correction2**(0.5)) / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, prune*exp_avg, denom)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-de7oKsIWeB",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRX1OD_IWeC",
        "colab_type": "code",
        "outputId": "9b7c81cd-09a0-4bf1-ca1c-d2fe12ae0b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47AcefhNIWeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Pte2nfIWeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN = TensorDataset(X_train.to(device), Y_train.to(device))\n",
        "TEST = TensorDataset(X_test.to(device), Y_test.to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAYSy-dZIWeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogReg(input_dim=X_train.shape[1], output_dim = 10, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcykU8MDIWeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXz5GEQ9IWeI",
        "colab_type": "code",
        "outputId": "558bc9c6-5e10-457e-d17a-dadd6bfb5894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainer(count_of_epoch = 100,\n",
        "        batch_size = 64,\n",
        "        dataset = TRAIN,\n",
        "        model = model,\n",
        "        loss_function = loss_function,\n",
        "        optimizer = optimizer,\n",
        "        progress = tqdm\n",
        "       )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [00:58<01:26,  1.47s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n2QdIc7-FnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'train_model.sv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An5G7SSRIWeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = model(TRAIN[:][0])\n",
        "\n",
        "loss = loss_function(output, TRAIN[:][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng4dPeRmtEln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_hess(model, hess):\n",
        "    bias = 0\n",
        "    new_hess = []\n",
        "    for p in model.parameters():\n",
        "        p_size = torch.tensor(p.size()).prod()\n",
        "        new_hess.append(hess[bias:bias+p_size].view_as(p))\n",
        "        bias+=p_size\n",
        "    return new_hess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYNkYmHpnLAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1a1dd4d-a3d8-44aa-d405-6ee0a1b0a3cc"
      },
      "source": [
        "%%time\n",
        "hess = hessian(loss, model.parameters())\n",
        "\n",
        "hhs = torch.diag(hess)\n",
        "hs = resize_hess(model, hhs)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 24min 2s, sys: 4.63 s, total: 24min 7s\n",
            "Wall time: 24min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jcPYBa7_NlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('hess.pkl', 'wb') as f:\n",
        "    pickle.dump(hs, f)\n",
        "\n",
        "# with open('hess.pkl', 'rb') as f:\n",
        "#     hs = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xobJxrVP_H6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('train_model.sv')\n",
        "files.download('hess.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp0v1YVi_gqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "7473626a-7d04-407c-ef9e-295132ebe64d"
      },
      "source": [
        "amounts = np.linspace(0, 1, 10)\n",
        "k_for_averaging = 5\n",
        "\n",
        "List_of_random_score = []\n",
        "for amount in tqdm(amounts):\n",
        "    List_of_score = []\n",
        "    for _ in range(k_for_averaging):\n",
        "        model.load_state_dict(torch.load('train_model.sv'))\n",
        "        optimizer = Adam(model.parameters())\n",
        "        optimizer.prune(amount=amount, method='random', hessian=hs)\n",
        "\n",
        "        trainer(count_of_epoch = 2,\n",
        "            batch_size = 64,\n",
        "            dataset = TRAIN,\n",
        "            model = model,\n",
        "            loss_function = loss_function,\n",
        "            optimizer = optimizer\n",
        "        )\n",
        "\n",
        "        output = model(TEST[:][0])\n",
        "\n",
        "        List_of_score.append(loss_function(output, TEST[:][1]).item())\n",
        "    List_of_random_score.append(List_of_score)\n",
        "  \n",
        "List_of_random_score = np.array(List_of_random_score)\n",
        "\n",
        "\n",
        "List_of_OBD_score = []\n",
        "for amount in tqdm(amounts):\n",
        "    model.load_state_dict(torch.load('train_model.sv'))\n",
        "    optimizer = Adam(model.parameters())\n",
        "    optimizer.prune(amount=amount, method='OBD', hessian=hs)\n",
        "\n",
        "    trainer(count_of_epoch = 2,\n",
        "        batch_size = 64,\n",
        "        dataset = TRAIN,\n",
        "        model = model,\n",
        "        loss_function = loss_function,\n",
        "        optimizer = optimizer\n",
        "    )\n",
        "\n",
        "    output = model(TEST[:][0])\n",
        "\n",
        "    List_of_OBD_score.append(loss_function(output, TEST[:][1]).item())\n",
        "  \n",
        "List_of_OBD_score = np.array(List_of_OBD_score)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 1/5 [00:14<00:57, 14.45s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [00:28<00:43, 14.47s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [00:43<00:29, 14.54s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [00:56<00:14, 14.14s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [01:09<00:00, 13.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 1/5 [00:03<00:12,  3.04s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [00:06<00:09,  3.07s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [00:09<00:06,  3.05s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [00:12<00:03,  3.02s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [00:14<00:00,  2.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYU8pQfjBKrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_mean = np.mean(List_of_random_score, axis = 0)\n",
        "random_std = np.std(List_of_random_score, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSJPNaJv9Z4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "19f929de-65a0-40bd-99e1-08f41433e495"
      },
      "source": [
        "plt.plot(amounts, random_mean)\n",
        "plt.fill_between(amounts, random_mean-random_std, random_mean+random_std, alpha=0.3)\n",
        "\n",
        "plt.plot(amounts, List_of_OBD_score)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6e90080358>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxcZ33v8c9vNs2MdlmbrcWO7Tib\nAyFRNjsJiR0gG05bUvaW9IampYTCpS2Xwr0tpLSvphtL2ZoGSoAAARIgZClbKImdxMSO4yS24y22\n5U3raJsZzXbmuX+c0ViWZWskjXRmRr/366WXZuYczfyORvrOc57znOeIMQallFLFz+V0AUoppfJD\nA10ppUqEBrpSSpUIDXSllCoRGuhKKVUiNNCVUqpE5BzoIuIWkW0i8ugky24XkV4ReTHz9f78lqmU\nUmoqnmms+2FgF1B1muUPGmPumn1JSimlZiKnQBeRVuBm4O+Bj+bjhevr682yZcvy8VRKKbVgbN26\ntc8Y0zDZslxb6J8DPgZUnmGdt4nINcAe4H8bYw5PXEFE7gTuBGhvb2fLli05vrxSSikAETl0umVT\n9qGLyC1AjzFm6xlW+ymwzBjzOuAXwP2TrWSMudcY02GM6WhomPQDRiml1AzlclB0LbBBRA4C3wPW\nici3x69gjOk3xsQzd+8DLslrlUoppaY0ZaAbY/7aGNNqjFkGvBN40hjz3vHriMjicXc3YB88VUop\nNY+mM8rlJCJyN7DFGPMI8OcisgFIASHg9vyUp5RSKlfi1PS5HR0dRg+KKqXU9IjIVmNMx2TL9ExR\npZQqERroSilVIoov0MM98MT/gVR86nWVUmoBKb5AP7QJNn8VfvxnkE47XY1SShWMGY9yccwFvwv9\n++HJv4PqFnjT3U5XpJRSBaH4Ah3g6r+A4aOw6fNQ1QqX3+l0RUop5bjiDHQRuOlfYKQLnvgYVC2G\n897qdFVKKeWo4utDH+Nyw9u+Bi2XwEPvh87NTleklFKOKt5AB/AF4d0PQtUS+O47oG+v0xUppZRj\nijvQAcrr4b0PgcsD3/49GOl2uiKllHJE8Qc6QN1yu6Ue6YPv/D7ER5yuSCml5l1pBDrYfem//w3o\negW+/z6wkk5XpJRS86p0Ah1g1Vvgls/C/l/BTz8MDk08ppRSTijOYYtncsn77DHqv7kHqlvhuk84\nXZFSSs2L0gt0gGv/GoYyoV61BC653emKlFJqzpVmoIvAWz8H4S549KNQudjujlFKqRJWWn3o47m9\n8Pv3Q/Nq+MHtcPRM17hWSqniV7qBDlBWAe/+gT1W/YG3Q+g1pytSSqk5U9qBDlDZBO99GIwF336b\nPVZdKaVKUOkHOkD92fCuB2H4GHznHZCIOl2RUkrl3cIIdID2y+Ft99l96Q/dAVbK6YqUUiqvFk6g\ngz3F7o3/BLsfhyf+Sk88UkqVlNIctngml98Jw0cyF8dogWv+0umKlFIqLxZeoAOs/5Tdn/7k39mh\nftG7nK5IKaVmbWEGussFt37JvuLRI3fZI2FWrHO6KqWUmpWF1Yc+nqcM3vkA1J8DD/4hHH/J6YqU\nUmpWcg50EXGLyDYReXSSZWUi8qCI7BORzSKyLJ9Fzhl/NbznB+Cvggd+HwY7na5IKaVmbDot9A8D\nu06z7A5gwBizEvgscM9sC5s31S3wnh9CchS+fRtEQ05XpJRSM5JToItIK3AzcN9pVrkVuD9z+4fA\nehGR2Zc3T5rOt7tfBg7A994DyZjTFSml1LTl2kL/HPAxIH2a5S3AYQBjTAoYAhZNXElE7hSRLSKy\npbe3dwblzqGzrobf+Qp0PgM/uhPSp9tUpZQqTFMGuojcAvQYY2Y9XaEx5l5jTIcxpqOhoWG2T5d/\nF94Gb/4M7PwJ/PyTTlejlFLTksuwxbXABhG5CfADVSLybWPMe8etcxRoA46IiAeoBvrzXu18uPIu\nGDoCz33ZHqO+5i6nK1JKqZxM2UI3xvy1MabVGLMMeCfw5IQwB3gEeF/m9m2ZdYrzvHoReMs/wHkb\n7Fb6Kw85XZFSSuVkxuPQReRuEdmQufs1YJGI7AM+Cnw8H8U5xuWG3/tPaL8SfvSncHCj0xUppdSU\nxKmGdEdHh9myZYsjr52zaAi+/hYId8P/+hk0nud0RUqpBU5EthpjOiZbtnDPFM1FsM4eo+7x2xfH\nGD7mdEVKKXVaGuhTqV1qh3psyD6bNDbkdEVKKTUpDfRcLH4dvP2b0PsqPPgHkEo4XZFSSp1CAz1X\nK9fDhn+HA7+Bn3xQTzxSShWchTl97kxd9G4YPgpPfsaeA+b6TzldkVJKZWmgT9fVfwlDR2HjZ+0T\njy77Y6crUkopQAN9+kTgpn+xL47x+F9B5WI47xanq1JKKe1DnxG3B277GrRcDA/dAZ2bna5IKaU0\n0GfMVw7v/j5ULYHvvgP69jpdkVJqgdNAn43yenjvQyBu+8SjkW6nK1JKLWAa6LNVt9xuqUd64Ttv\nh3jY6YqUUguUBno+tF4Ct/0XdL0EP3gfWEmnK1JKLUAa6Plyzg1w87/Bvl/Cox+BApk92BjDYDRB\nsc5mrJTKnQ5bzKeOP7JPPHrqn6GqFa77a8dKSVppjg2Ocjg0SixpUeZ10VIToKU2QJnH7VhdSqm5\no4Geb9d90p6V8Tf/aJ9NevEfzuvLRxMpDodGOTY0imWdaJXHk2le641wsD9CY6Wf1toANUHfvNam\nlJpbGuj5JgJv/TyMHIeffgQqmmHVm+f8ZQciCTpDUfrC8TP29qTT0DUUo2soRqXfQ2tdkOYqP26X\nzHmNSqm5pX3oc8HttWdnbLrAPkh69IU5eZl02nB8aJTNr/Wz9dAAvSNnDvOJRmIpdh0b5um9vezt\nHmE0Yc1JnUqp+aGBPlfKKuE9P4BgvT2cMXQgb0+dSKU50Bdh0/4+dhwdZiSWmtXzpSzDof4oz+zv\n48XDg/SH43mqVCk1nzTQ51Jls33iUTpln3gU6Z/V00XiKXYdH2bTvj7294SJJ/M7ha8x0DcSZ1vn\nIM/s66OzP0rS0mmClSoWGuhzrWEVvOt7MHTEniIgEZ32U/SH47zQOcCz+/s5OjCKlZ77IYjRhMWe\n7hE27utj1/FhwvHZ7QUopeaeBvp8aL8C3nYfHNkCD70f0lP3VafThqODozy7v59tnYOEws5cJcmy\nDEcHRnlufz9bD4XoGY7pmHalCpSOcpkv52+AG++BJz5mT7t787/aI2ImiKcsDodGOTo4SjJVWN0d\nA5EkA5EhyrwuWmuDtNQE8Hm0TaBUodBAn0+X/4nd9fLMF6C6Fa7+aHbRSCzJof4oPSOxgr+6XTyZ\nZn9PmAN9YRor/bTVBqkOep0uS6kFTwN9vl3/afts0l99GlO5mN4Vv8vhUJSBSPHN/zJ+THtVwEtr\nbYDmKj8uHdOulCM00Oeby0XqrV8iNdiF7yd3ceSaMgaa1jpd1awNjybZOZpkb0+Ylho/rbVB/F6d\nYkCp+TRlB6iI+EXktyKyXUR2iMinJ1nndhHpFZEXM1/vn5tyi1ssabG3e4SNB0d4ruPzRKuW87pN\nd1ExuMvp0vImmUpzsC/Kpn19bNcx7UrNK5lqxIKICFBujAmLiBfYCHzYGPPcuHVuBzqMMXfl+sId\nHR1my5YtM6u6yAxFk3SG7P7x8b/usuhxLv3V2wHD8+u+T7x8iWM1zqVgmZu22iCLq/143HoQVanZ\nEJGtxpiOyZZN+d9lbGNXbfBmvnTc2hSMMXQPx3j+YIjnD4boHo6dclp+PLiYbVffhycV5Q1P34En\nMeRMsXMsGrfY3TXC0/v6eLVrmIiOaVdqTuTUXBIRt4i8CPQAvzDGTHZV5LeJyEsi8kMRactrlUUk\naaU51B9h075+Xj4yxFD0zAc7IzXnsH3tlwmGO3n9pj/DZZVuF4VlGY6E7LH1Ww8NZPZYtG2gVL7k\nFOjGGMsYcxHQClwmIqsnrPJTYJkx5nXAL4D7J3seEblTRLaIyJbe3t7Z1F1wookUu7vsMyv3doeJ\nJXOf6Gqg8Qp2XHYPtb3Pc/5vPwamwMct5sFAJMFLh4fYtK+fg30REgU25l6pYjRlH/opPyDyN0DU\nGPMvp1nuBkLGmOozPU+p9KHnOm1tLpa+eh9nv/RPHFp1O3sv+kR+CiwSLhc0VflpqwtS5dcx7Uqd\nzpn60KcctigiDUDSGDMoIgHgTcA9E9ZZbIw5nrm7ASidYRuTSKcN3SMxOvujs57pcLxD59xBWfQ4\nS/d8g3hgMZ3n/FHenrvQpdNwfDDG8cEY1UF7THtTpY5pV2o6chmHvhi4P9PydgHfN8Y8KiJ3A1uM\nMY8Afy4iG4AUEAJun6uCnZRIpTk6OMqRgWjeZzoEQIQ9F30C/2g3Z2//R2LBZnrabsz/6xS4oWiS\noWiSvZ4wS2oCtNYGdEy7UjmYdpdLvhRTl0s4nqKzP0r3cGxeZjp0pWJc/NTtVIVe5oU3foPBhkvn\n/DULmQg0VJbRVhuktlwvm1dIklaa0aRFhc+je1Pz5ExdLhroZ9AfjnMoFHVkpkNvfICOJ9+JL9bP\nlnXfI1K9ct5rKETlZR7a6gIsrg7oZfPmSdJKE01YjCYsoomUfTtpEYmnSGWuW+txCw2VZTRV+akL\n+jTc55AG+jRYmcu6HQ6NOj5e2h85wqW/egdpl5fn1z9IItDkaD2FxOOWbHdM0KczWMxWykoTTY6F\nth3cowmLSMKa9qyfHrfQWOmnqaqMunIfMsmsomrmNNBzEEtaHBkovGlrKwd2cMmv38toeStb1n0X\ny1vhdEkFp67CR1ttkPoKDY8zsdImG9TRccEdTVhzNmzU53HRWFVGU6WfmqBX35880EA/g+FYks4C\nn7a2rutpLnr6TxhovIwXr7oX49Z+5MkEfG5aawMsqQngXaBTDKTThmjSmjS45+RA/jSUeV3ZlntN\nUP+GZ0oDfQJjDL3heFFNW7v4wMNc8PzHOb70VnZc9k+TXhxD2dwuyYxpD1BZgmPa02nDaNI60a+d\ntFvZ0bhFPGXN+nyI+eD3ummqKqOxyk91oPTeo7k0q3HopSRlpTk+FONwKEo0kfuZnIXg+Fm/h3+0\nixWvfI5YoJn9r/sLp0sqWFbacGxwlGODo9QEvbTWBmmsLCuqA3XGTAjthEUk0+qOJYsjtM8klrQ4\n1B/lUH+UgM8O96Yqf0l+AM+nBRHosaTF4VCUo4Oj2aPyxejAeR+gLHqMs179D2LBxRxd+W6nSyp4\ng9Ekg1H7snlLagK01BTOmHZjDLFk+pSRI6MJi1jKKtguwHwbTVgc7ItysC9K0OemqdpPU5WfirIF\nEU95VdK/sdNNW1u0RNh98acoG+3l3G13kwg00ttyvdNVFYV4Ms2B3giH+iM0VNjdMfPVjxtLnnwA\ncux2LLlwQjtX0YTFgd4IB3ojlJd5aKoqo7naryOZclRyfejGGHpG4nSGolPOdFisXKkol/zPH1Ax\ntJet136T4UUXOV1SUarwe2irC9Jc5Z/1mPbY2JC/pEU0fiK4R5MpDe08qPB7aK6yW+4BX2HsYTll\nQRwUTVppjg3a48enM9NhsfLG+rn0yXfgSQyzZf2DRCvPcrqkouVxCy01AVprg2cMi3jq5HHa2dBO\nWPNyBrGyVQW82T73Quk+m08lHejRRIrDoVGODY1iFXH/+EwERg5x6ZNvJ+UpZ8v675Pw1ztdUlET\ngUUVZSyp9mMZQyQ+7uzIpLXg/r6KQU3QS1OVn4bKsgUT7iUZ6PmctraYVfVv55L/+QPCVSt54dpv\nYXnLnS5JqXknYoe7Pc7dj89TuuchzOoSdIVoX0+YrYcG6B1Z2GEOMLzo9bx85eeoGtzJhc99BEnr\n5d3UwmMMDESS9qUO9/byQueAfda3tbAOYBRloKf0KNNJ+pas49WLP0X98d9w7ta/ZcF/yqkFzRgI\nhRPsOjbM03t72dY5wLEFEu46FqhEHF3xTvzR45y16yvEgos5cMFdTpeklOPSaegPJ+gPJ3C5YFG5\nfTC1vsKHpwSnh9BALyH7V3+EsuhxVuz4AvFAM8eW3+Z0SUoVjHQaekfi9I7EcbuERRW+TLiXlcxU\nzBropUSEXR2foSzWy7lb/x/xQAP9i9/odFVKFRwrbegZjtMzbId7Q2UZjVVl1JcX1xQRE5XePscC\nZ9w+Xlrz70SqV3Hhsx+mMvSK0yUpVdCstKFrKMZLh4f4zd5eXjk6RO9InHQRnluggV6CLG8F267+\nT5K+Gi7aeCf+8GGnS1KqKFiWHe7bDw/y1N5edh4bpj8cx6nh3dOlgV6iEoFGtl1zH650kjc8/X4q\nBl91uiSlikrKsmft3NY5yFN7+9h1fJhQJFHQ4a6BXsKiVSt5ce1X8MX6uOLnG7jwmT+nfGiP02Up\nVXSSqTRHB0Z54dAAT+/tY3fXCIPR+b/W8FT0oGiJG2roYNPNT9K++79o33s/jUd+RnfbDRw4/0N6\n4WmlZiCRSnM4FOVwKEqZ10VTZtKwQrhQR1Ge+v9q1zBHQqN5rqj0eeMDtO/+Om37voU7NUp3+828\ndv4HiVatcLo0pYre2IU6Gqv8VM3hhTpKbi4XDfTZ8cZCLN39Ndr2fRtXOk5X+y28dv5djFYuc7o0\npUpC0Oemscq+fmq+r8Kkga4m5Y31s3T3fbTtewBJJ+lauoED53+Q0Yp2p0tTqmSMXaijqcpPeR6u\nwqSBrs7IN9rL0t3/Sev+7yLpFMeX/S4HzvsAsYo2p0tTqqRU+D201tpz78/UrGZbFBG/iPxWRLaL\nyA4R+fQk65SJyIMisk9ENovIshlXq+ZdItDA3os+waabfsWRle+h+dAjrHniLZy75f/ijxx1ujyl\nSkY4luL4UGzOnj+XYYtxYJ0x5vXARcANInLFhHXuAAaMMSuBzwL35LdMNR8SgUb2vOH/8sxNv+To\ninew5OCPWPPEmzl3699SFj3udHlKqSlMGejGFs7c9Wa+JvbT3Arcn7n9Q2C9iBTvhAgLXDzYzO6L\n/5ZNN/2SY2fdxpIDP2Tt49dzzgufpiza5XR5SqnTyOnEIhFxi8iLQA/wC2PM5gmrtACHAYwxKWAI\nWDTJ89wpIltEZEtvb+/sKldzLh5czKuXfJpnbvw5x5b9Li37H2TN49ezattn8I32OF2eUmqCnALd\nGGMZYy4CWoHLRGT1TF7MGHOvMabDGNPR0NAwk6dQDoiVt/Bqx2d45saf0bV0A637HmDt4+s5+8V/\nwBfrc7o8pVTGtE79N8YMAr8Gbpiw6CjQBiAiHqAa6M9HgapwxCra2HXpP/DsjT+ju+1m2vd+k7WP\nrWPl9nvwxvTtVsppuYxyaRCRmsztAPAmYOJMT48A78vcvg140hTyDDZqVkYr2tl52T/yzA3/TU/r\nW1i657+46rF1rNz+z3jjIafLU2rByqWFvhj4tYi8BDyP3Yf+qIjcLSIbMut8DVgkIvuAjwIfn5ty\nVSEZrVzGjsv/mWff8jg9LdezdPd9rH1sPSte+le88QGny1NqwdETi1TelA/t46ydX6Tp8BNYniCd\nZ7+PznP+iJSv2unSlCoY1UEvly6rm/HPz+rEIqVyFaleyStXfo7n3vJT+puvZvmuL3PVY9ex/JUv\n4EkMO12eUiVPA13lXaR6FS+v+QLPvfkRQo1XsnznF1n72DrO2vEl3Mnw1E+glJoRDXQ1Z8I15/LS\n2i+x+U0/ZrDhUlbs+DxXPXYdy3Z+RYNdqTmgga7m3Ejt+Wy/6itsvv5hBhddzMpXPsvax9axdNe9\nuJMRp8tTqmRooKt5M1K3mu1X/we/Xf8Dhutex9kv/wtrH19H+6tfw5WKOl2eUkVPA13Nu+FFr+fF\na+7j+XUPMlJzAateuoe1j62nfffXcaV09JJSM6WBrhwzVP8Gtr3x6zy/7ruEq1exavs/svbx62nb\ncz8uK+50eUoVHQ105bih+kvYdu39bLnuASKVyznnxb9nzePrad37LQ12paZBA10VjMGGS3nhum+x\n9dpvMlrezrnb/o41j7+Jln3fQayE0+UpVfA00FXBGWi8gq3XPcDWN36DWHAJ573wKdY88WZa9n9P\ng12pM9BAV4VJhIGmNWxZ911euObrxAONnLf1b1jz3zew5LUfIOmk0xUqVXA00FVhEyHUfBVb1j3I\ntqv/k0RZLedv+SRXPnEDiw88jKRTTleoVMHQQFfFQYT+xW/k+fU/5MWr/oOUr4oLnv84V/73jTQf\n/LEGu1JooKtiI0Lfkuv47fUPs33tl7E8QVb/9mNc8bObaTr0U0hbTleolGM00FVxEqG35Xo2v+lH\nbF/zRdIuHxdu/guu+PktNHU+CibtdIVKzTsNdFXcxEVv65vZ/Oaf8NKVXwBcXPjcR7niZ2+l8fAT\nGuxqQdFAL3HGGKKJFCV/RUBx0dN2A8+95ae8fMVngTSve/bDXP7zW2k48jMNdrUgeJwuQOVXykrT\nORBlX0+Y/b0R9veEGRxNUhv0sqqpknOaKlnVXElTZRki4nS5+Scuuttvprv1BpoOP87ynV/k9c98\niJGa83jtgg/Ru2Q9lOJ2K4UGetELx1Ls7wtnAjzMgb4ISctujS8q93FOcyVLagIcDkXZeXyYzQfs\nizhXB7ysaqrIhvzian9pBbzLTffSt9LTdiNNnY+yfOeXeP2mP2O4djWvXfAh+hZfq8GuSo4GehEx\nxtA1HGN/T4R9vWH29YbpGooB4BahrS7AG1c1sLKhghWNFdQGfZP+/J7uMHu6R9jTPcLzB+2LOVeU\neU4K+JbaAK4SCDzj8tC17Hfobr+F5kOPcNbOL3HRxj9hqO5CXrvgz+lvvkaDXZUMvUh0AYunLA71\n290n+3rD7O8JE0nYw/LKfW5WNFawsqGClY0VLF0UpMzjntbzG2PoDcfZ0xVmdybg+yP2qfVBn5tV\njZWsarZDvr02iMtV/MEn6SSLD/6Es3Z9mUDkCEN1r2f/6g8TalqrwT4ZY5B0Elc6gctK4ErHM98T\n2cfEpIhUrSRZVut0tUVhLi8SrYFeQAajCbvlnen/7uyPYmXen+YqPysbK1jRUM7Kxgqaqvxz0oLu\nD8fZ030i4HtG7NkOA143KxrL7T74pkqWLgricRXvMXWxEiw++CPO2vUVAtFjDC66mNdWf4hQ4xrn\ngz1tjQvM+LgwHXc/nUCsxIRlp64z8TGZbP3ThLVYCdzp3ObOMQjDdasJNV1Ff9NahhZdhHH7pv7B\nBUgDfYJSCPR02nBkcJT9Y63v3jB9Yfufx+sWzqovZ0Wm9b28vpxKv9eROgeiiUz3jB3yY108ZR4X\nKxoqWNVUwTlNlSyrL8frLr6AFyvBkgMPcdaur+Af7WKgvoMDF3yQ0fK2U0JOJgu+SVutyexjcrpg\nzj6WzK7ntuL2+iY/J0elxY1x+Ui7faRdPtLuMtIub+a2j7SrLLvMTPJY+qSfHf+YvY7J3DciVPdv\nZ1H3Rqr6t+MyFilPkIGGywk1raW/eS3RyuXOf1AWCA30CYox0KOJFAf6Itnuk9d6I8RT9lC66oD3\npNZ3e20QT4GG49Bokr09I9lumqOD9vvgdQvL6zMB31zJ8voKfJ7C3IbJiJWg5bXvs+zVr+If7Znx\n86Rd3kxwTgzEyR47EYqnD9Nxoev2YWW+n26d8c9rXPN/iMydGKGudzN1XRtZ1L2JYPgQALHgYvqb\n1hJqWkuo6UqSZTMPtGKngT5BoQe6MYa+8PjukzBHB0Yx2I2U1ppAJsDtFviicl/RjjAJx1Ls7RnJ\ndNGEORyKYgCPy97LWNVUyaomu6+/zDu9Pn4nuKw49cd+hduKY7nLphe4Lq+2Qifwhw+zqHsTdd2b\nqOt+Fm9yGIMwUnuBHfDNVzG46A0LqnvG0UAXkTbgm0ATYIB7jTGfn7DOtcBPgAOZhx42xtx9puct\npUBPWmk6Q9FseO/vjTA0ak/v6ve6WFFfkT2AeVZ9OQFf4QfbTEUTKfb2hNnTNcKenjCH+iOkjT0K\nZ+miYDbgz26sLOnfg5pE2qJq4BUWdW2krnsj1f3bcZkUljvAQONl9DddRahpLZGqFSX9weh0oC8G\nFhtjXhCRSmAr8DvGmJ3j1rkW+EtjzC25FlXMgT4SS7K/N3LS2O9U2v49NlSUsaLxRP93S3WgJEaH\nzFQsabGvxx4mubt7hIP9Uay0QQTa64LZYZJnN1ZQXqajaBcSdzJMbc9mFnVvpK77GcpH7PZgLNBk\nH1xtXkuocQ1Jf2l1z8xloE/5H2SMOQ4cz9weEZFdQAuw84w/WCLSxtA1FDsxdLA3TPewPfLD7RKW\n1gW57txGe+x3Qzk1wYWz65gLv9fN6pZqVrdUA/ZQzNd6I9mA//WrPfxiZzcCtNYGMi14uxXv1IFg\ndaqklWYwmmQgmmAgkiAUTTCQuT8UTVJX7qO9Lkh7XZC2uiDVganfO8tbQV/Levpa1gPgjxyhrmsT\ni7qfoeHYL1ly8CEAhmsvINS0hv6mqxisv2RBdc9M17T60EVkGfAUsNoYMzzu8WuBh4AjwDHs1vqO\nMz1XobbQ40mLA/2RE6fO94aJZsZ+V5R5MiftlLOyoYKli8qL6sBfIUpaaQ70Rew++K4R9vdGSFj2\nweIl1f5swJ/TXJlTSKjpS1rpTFAn7aCOJOz7mcAORRKMxE6dbz7gdVNb7qXK76U/nKA3fOKC3tUB\nL0vHBXx7XZD6imkcK8p0z9R1P8Oi7o1U923LdM/4GWi4NNOCv4pI1cqi654piIOiIlIB/Ab4e2PM\nwxOWVQFpY0xYRG4CPm+MOXuS57gTuBOgvb39kkOHDk1vSzLyGeihSIL94w5edoaiZHpPWFLtz3ad\nrGisKN35TwpIykpzsD+aPZN1b084OxqoqaosOw5+VVMldeXaUptKPGWNa1mfGtihSIJw/NSwDvrc\n1AZ91JZ7qQv67NuZ+2O3Jx4DiSZSHA6NcigU4XBolM5QlONDo9n/p6DPTVttMNuSb68L0lztx51D\nl6Q7Gaa29/nM6JmN47pnGu2hkU1XEWpaQ9K/aPa/tDnmeKCLiBd4FPiZMebfclj/INBhjOk73TpO\ntNCttOHIuImr9vWGCWXOjPS5XfbY70zre3lDBRXap+s4K23oDEWzXTR7u8OMJu09pvoK34kJx5oq\np9cCLAHxpHWiFZ0N6uRJgShM4k0AAA5bSURBVD1ZWJf73NSW++ygLvdRG7RDuq7cDuqaoBd/nkYk\nJVJpjg7a4d4ZinI4FOXwQDQ735DXLbSOC/m2ugCtNcEp93zLIscyo2c2Utf9LL7EIAAjNefZ4d68\nhsH6DtLusrxsRz45fVBUgPuBkDHmI6dZpxnoNsYYEbkM+CGw1Jzhyecj0KOJFK/1nhj7faDvxNjv\n2qA32/pe2VBBa12gqM98XCjSacORgdHsmax7ukey0yHUBX3ZqQpWNRX3jJKxpEVoQtdHtu86Yt8f\n6wocr6LMYwf0aQK7Juid9hQR+Wal7TmFxod8Zyia3R6XQHO1/6SWfHtdkKDvNA2stEXl4M7M6Jln\nqOl/AVc6ieUuY6Dhsmz/e6R6VUF0zzgd6FcBTwMvA2OTSn8CaAcwxnxVRO4CPgCkgFHgo8aYZ870\nvPkOdGMMPSPxcd0nEY4N2mO/XQKttcHsvCcrGspZVFF4n9xq+tLGcHwwlg343d0j2f7eQp1RcjRh\nZfums4GdPdBoB/bYXsh4lX6PHczjuz7GBXZt0Fe0x3SMMfRHEtmQ7+y3vw9mhv+CvUc2MeSrA95T\n3lN3MkJN72+z498rhvcDEPc32OHefBWhxjUkAg3zuo1jHO9ymQuzDfQDvZHsxFX7MzMPjv0jB7zu\n7FmXKzJjv/O1C6kK2/gZJXd32SE/FgqVfo894VhTBauaK2mpye+MksYYRpPW5AcXxwV2LHnqxTaq\n/J5sl8dJfdfjukGKcWqF2RoeTZ4I+UxrvnvkxMHXSr8nG+5LMwdgGyrLTnpfy6LHWdQ1dnLTphPd\nM9Xn2OGeGT2T9vjnZZs00Md5Zl8fn3lsF3u6R7Jjvxsry07qPllcMzcTV6nik68ZJY0xRDIt6/F9\n1eNb1QPRRLZLb4wAVQEvtUHvaQO7JuAt2KkeCtFowuLwwMkhf2wwlp3Izu91nXLwdXGN3+5SNWkq\nB3ZS172JRd2bqOnbmu2eGazvyPS/ryVcfc6cdc9ooI+z5WCIv/nJDtpq7dPnlzdU6HA2NS394Xh2\nqoLd3SP0jptRcqxLLmGlsyE91spOTAxrgZqAd9IRIGOBXR306rGZeZC00hwbd/C1MxTl8MBo9j3z\nuIQlNYHsUMr2RUFaawIEJE5t7/OZFvxGKob3ARD312f73kNNa0gEGvNWqwb6BE6fKapKy2QzSroE\nagKTh/RYN0h1wJvTkDvljHTa0D0Sm9BlM5od+SMCTVV+2se15lcFRmgbPHH2qi9uX+FrpPqcbP/7\nYH0HaU9gxnVpoE+gga7mUixp4XO7FvSUDaXKGMNANHnKwddQ9MS872NnvS6t9XOJ/ygXJV6gJfQc\ntX1b7O4Zl4/Bho7M+Pe1hGvOBcl9L0wDfQINdKVUPoVjqZNa8p0DUbqHYoylY0WZh5W1wrrAPi5P\nb2dVZAu14Uz3TNkiQk1rCDXbAZ8INJ3xtRydy0UppUpdhd/D+UuqOH9JVfaxeNLiyOAoh/pPBP0X\nepaRSi8FNtDmGeSWit1c436FC49tZHHnTwEIV51tTyzWdBUDDZfOqntmujTQlVJqEmVeNysa7KHP\nY1JWmuNDJ/rlfxlq5b8GriSeTHGedHKN+xXWh3fw+j0PsHTPN7DEy0D9JQw026NnRmrOm9OatctF\nKaVmIW0MvSPxk7psekIDnJvYwdWul7nG9RLnug4DEHZX01V/JU1X307l6htn9Hra5aKUUnPEJUJT\nlZ+mKn+2b9wYw9DoG+gMRflqKMpI7xFaBjZzUWIbV3c9y/PPLmXdDAP9TDTQlVIqz0SEmqCPmqCP\n17XWAEuAy4jEUzwaiXP1WZVz8roa6EopNU/KyzwsqQ2wsmVursKkp7AppVSJ0EBXSqkSoYGulFIl\nQgNdKaVKhAa6UkqVCA10pZQqERroSilVIjTQlVKqRGigK6VUidBAV0qpEqGBrpRSJUIDXSmlSoQG\nulJKlQgNdKWUKhEa6EopVSKmDHQRaRORX4vIThHZISIfnmQdEZEviMg+EXlJRC6em3KVUkqdTi4X\nuEgBf2GMeUFEKoGtIvILY8zOcevcCJyd+boc+Ermu1JKqXkyZQvdGHPcGPNC5vYIsAtombDarcA3\nje05oEZEFue9WqWUUqc1rT50EVkGvAHYPGFRC3B43P0jnBr6iMidIrJFRLb09vZOr1KllFJnlHOg\ni0gF8BDwEWPM8ExezBhzrzGmwxjT0dDQMJOnUEopdRo5BbqIeLHD/AFjzMOTrHIUaBt3vzXzmFJK\nqXmSyygXAb4G7DLG/NtpVnsE+MPMaJcrgCFjzPE81qmUUmoKuYxyWQv8AfCyiLyYeewTQDuAMear\nwOPATcA+IAr8Uf5LVUopdSZTBroxZiMgU6xjgA/mqyillFLTp2eKKqVUidBAV0qpEqGBrpRSJUID\nXSmlSoQGulJKlQgNdKWUKhEa6EopVSI00JVSqkRooCulVInQQFdKqRKhga6UUiVCA10ppUqEBrpS\nSpUIDXSllCoRGuhKKVUiNNCVUqpEaKArpVSJ0EBXSqkSoYGulFIlQgNdKaVKhAa6UkqVCA10pZQq\nER6nC1A2kcwXkrktCCcecwkwfjngcp1Yh8zjrgk/Zy8DK21IWmmSlv09lU6TTju0sarguVzgcbnw\nul34PILX7cLjcuHzuPC67ftetwuf24XXI6SN/Tc29pU2hlTakE7b37OPWfZ3K/P4xMfGvtTMFGWg\n+z1uKv2enANt4u3x60z5c7kEq0h2OQKuSX7ulDrG/9xY6s6zEyGfJmUZkmk78FOZx+zbhoSVzjxm\nr2NZ+g9XTFwusgE8FsKeTCiPBfJky5xkB77d6LCMwbIMlpn8sfEfBFb69I+lF8CHRVEG+rL6cpbV\nlztdRtFzuwS3y43f657WzxljTrT0xwI/Pe62Ne6DIm1IptIk0/YHhSnt/6c553aNha/g9UwSzhOX\nuQSPw+E8E2N/m/lmzFShT/ZDI5VOZ/c0rLQ5+TFrwvqZ206bMtBF5OvALUCPMWb1JMuvBX4CHMg8\n9LAx5u58FqkKi4jg8wg+z/SD4qS9gVQm8K3xewYnuoTGf2iUYsvK7ZZsCHvG3R7r0rC7N07u4nC7\nnNmbKxUigsctc9KSNWZC91L6xF7B2AdGyjJzuveTy3Z9A/gi8M0zrPO0MeaWvFSkSpodTBBgeq2v\ndHpsT8AO/pP3BOxd8WTq1A+K1Dx1D2UD2WO3iicGcjasM8t9bhcuDeeSIiKZD1/napgy0I0xT4nI\nsrkvRanTc7kE/wx2wce6h8YH/sQuoUTq5A8KK21wi70H4pkkkMfuj1/m1HEQpcbL157HlSKyHTgG\n/KUxZkeenlepWcl2D+ECn9PVKDW38hHoLwBLjTFhEbkJ+DFw9mQrisidwJ0A7e3teXhppZRSY2bd\nO2+MGTbGhDO3Hwe8IlJ/mnXvNcZ0GGM6GhoaZvvSSimlxpl1oItIs2Q6EEXkssxz9s/2eZVSSk1P\nLsMWvwtcC9SLyBHgbwEvgDHmq8BtwAdEJAWMAu80RkcbK6XUfMtllMu7plj+RexhjUoppRxUfKeQ\nKaWUmpQGulJKlQgNdKWUKhEa6EopVSLEqQEpItILHJrhj9cDfXkspxjoNi8Mus0Lw2y2eakxZtIT\neRwL9NkQkS3GmA6n65hPus0Lg27zwjBX26xdLkopVSI00JVSqkQUa6Df63QBDtBtXhh0mxeGOdnm\nouxDV0opdapibaErpZSaQANdKaVKREEHuojcICK7RWSfiHx8kuVlIvJgZvnmUrhUXg7b/FER2Ski\nL4nIr0RkqRN15tNU2zxuvbeJiBGRoh/ilss2i8jbM+/1DhH5znzXmG85/G23i8ivRWRb5u/7Jifq\nzBcR+bqI9IjIK6dZLiLyhczv4yURuXjWL2qMKcgvwA3sB5ZjXzxsO3D+hHX+DPhq5vY7gQedrnse\ntvk6IJi5/YGFsM2Z9SqBp4DngA6n656H9/lsYBtQm7nf6HTd87DN9wIfyNw+HzjodN2z3OZrgIuB\nV06z/CbgCUCAK4DNs33NQm6hXwbsM8a8ZoxJAN8Dbp2wzq3A/ZnbPwTWj11so0hNuc3GmF8bY6KZ\nu88BrfNcY77l8j4D/B1wDxCbz+LmSC7b/MfAl4wxAwDGmJ55rjHfctlmA1RlbldjX6O4aBljngJC\nZ1jlVuCbxvYcUCMii2fzmoUc6C3A4XH3j2Qem3QdY0wKGAIWzUt1cyOXbR7vDuxP+GI25TZndkXb\njDGPzWdhcyiX93kVsEpENonIcyJyw7xVNzdy2eZPAe/NXEjnceBD81OaY6b7/z6lfFwkWjlARN4L\ndABvdLqWuSQiLuDfgNsdLmW+ebC7Xa7F3gt7SkQuNMYMOlrV3HoX8A1jzL+KyJXAt0RktTEm7XRh\nxaKQW+hHgbZx91szj026joh4sHfTivl6prlsMyJyPfBJYIMxJj5Ptc2Vqba5ElgN/I+IHMTua3yk\nyA+M5vI+HwEeMcYkjTEHgD3YAV+sctnmO4DvAxhjngX82JNYlaqc/t+no5AD/XngbBE5S0R82Ac9\nH5mwziPA+zK3bwOeNJmjDUVqym0WkTcA/4Ed5sXerwpTbLMxZsgYU2+MWWaMWYZ93GCDMWaLM+Xm\nRS5/2z/Gbp0jIvXYXTCvzWeReZbLNncC6wFE5DzsQO+d1yrn1yPAH2ZGu1wBDBljjs/qGZ0+EjzF\nUeKbsFsm+4FPZh67G/sfGuw3/AfAPuC3wHKna56Hbf4l0A28mPl6xOma53qbJ6z7PxT5KJcc32fB\n7mraCbyMffF1x+ue420+H9iEPQLmReDNTtc8y+39LnAcSGLvcd0B/Cnwp+Pe4y9lfh8v5+PvWk/9\nV0qpElHIXS5KKaWmQQNdKaVKhAa6UkqVCA10pZQqERroSilVIjTQlVKqRGigK6VUifj/aO62ffda\n6ucAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alT6D8JADOhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}